# ğŸš€ GuÃ­a de InstalaciÃ³n - LA AURORA

## Sistema de Reuniones Ejecutivas con IA Local

Esta guÃ­a te ayudarÃ¡ a instalar y configurar **LA AURORA**, un sistema completo de transcripciÃ³n y anÃ¡lisis de reuniones que utiliza modelos de IA 100% locales.

---

## ğŸ“‹ Requisitos Previos

### Sistema Operativo
- **macOS** (recomendado)
- **Linux** (Ubuntu 20.04+)
- **Windows** (con WSL2)

### Hardware MÃ­nimo
- **RAM**: 8GB (16GB recomendado)
- **Procesador**: Intel i5/AMD Ryzen 5 o superior
- **Almacenamiento**: 10GB libres
- **GPU**: Opcional (acelera transcripciÃ³n)

---

## ğŸ› ï¸ InstalaciÃ³n Paso a Paso

### 1. Instalar Node.js

#### macOS (con Homebrew):
```bash
brew install node
```

#### Linux (Ubuntu/Debian):
```bash
curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
sudo apt-get install -y nodejs
```

#### Windows:
Descarga desde [nodejs.org](https://nodejs.org/)

### 2. Clonar el Repositorio

```bash
git clone https://github.com/joelosiris11/la-aurora.git
cd la-aurora
```

### 3. Instalar Dependencias

```bash
npm install
```

### 4. Instalar Python y Dependencias

#### macOS:
```bash
brew install python3
pip3 install openai-whisper torch torchaudio
```

#### Linux:
```bash
sudo apt update
sudo apt install python3 python3-pip ffmpeg
pip3 install openai-whisper torch torchaudio
```

### 5. Instalar Ollama

#### macOS/Linux:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Windows:
Descarga desde [ollama.ai](https://ollama.ai/download)

### 6. Configurar Modelos de IA

#### Descargar Modelo de Ollama (por defecto):
```bash
ollama pull gemma2:2b
```

#### Descargar Modelo de Whisper (por defecto):
```bash
# Se descarga automÃ¡ticamente en el primer uso
# Modelo por defecto: "base"
```

---

## âš™ï¸ ConfiguraciÃ³n de Modelos

### ğŸ¤– Cambiar Modelo de Ollama

Puedes usar cualquier modelo compatible con Ollama:

#### Modelos Recomendados:
```bash
# Ligeros (2-4GB RAM)
ollama pull gemma2:2b
ollama pull phi3:mini

# Medianos (8-16GB RAM)
ollama pull llama3.1:8b
ollama pull mistral:7b

# Pesados (16GB+ RAM)
ollama pull llama3.1:70b
ollama pull codellama:34b
```

#### Cambiar en el CÃ³digo:
Edita `server.js` lÃ­nea ~15:
```javascript
const OLLAMA_MODEL = 'gemma2:2b'; // Cambia aquÃ­ tu modelo
```

### ğŸ¤ Cambiar Modelo de Whisper

#### Modelos Disponibles:
- `tiny` - MÃ¡s rÃ¡pido, menos preciso (39MB)
- `base` - Balanceado (74MB) **[Por defecto]**
- `small` - Mejor precisiÃ³n (244MB)
- `medium` - Muy buena precisiÃ³n (769MB)
- `large` - MÃ¡xima precisiÃ³n (1550MB)

#### Cambiar en el CÃ³digo:
Edita `server.js` lÃ­nea ~16:
```javascript
const WHISPER_MODEL = 'base'; // tiny, base, small, medium, large
```

#### Cambiar Idioma:
Edita `server.js` lÃ­nea ~17:
```javascript
const WHISPER_LANGUAGE = 'es'; // es, en, fr, de, it, pt, etc.
```

---

## ğŸš€ Ejecutar la AplicaciÃ³n

### MÃ©todo 1: Script de Inicio
```bash
chmod +x start.sh
./start.sh
```

### MÃ©todo 2: Manual
```bash
node server.js
```

### MÃ©todo 3: NPM Script
```bash
npm run aurora
```

---

## ğŸŒ Configurar Acceso PÃºblico (Opcional)

### Instalar Cloudflare Tunnel

#### macOS:
```bash
brew install cloudflared
```

#### Linux:
```bash
wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb
sudo dpkg -i cloudflared-linux-amd64.deb
```

### Crear Tunnel:
1. Visita [Cloudflare Zero Trust](https://one.dash.cloudflare.com/)
2. Crea un tunnel
3. Copia el token
4. Ejecuta:
```bash
cloudflared tunnel --protocol http2 run --token TU_TOKEN_AQUI
```

---

## ğŸ“ Estructura del Proyecto

```
la-aurora/
â”œâ”€â”€ server.js              # Servidor principal
â”œâ”€â”€ package.json            # Dependencias Node.js
â”œâ”€â”€ whisper_transcriber.py  # Script de transcripciÃ³n
â”œâ”€â”€ public/                 # Frontend
â”‚   â”œâ”€â”€ index.html         # Selector de modos
â”‚   â”œâ”€â”€ index-pro.html     # Modo profesional
â”‚   â”œâ”€â”€ automatic.html     # Modo automÃ¡tico
â”‚   â”œâ”€â”€ resumen.html       # AnÃ¡lisis avanzado
â”‚   â”œâ”€â”€ styles.css         # Estilos principales
â”‚   â”œâ”€â”€ components/        # Componentes JavaScript
â”‚   â””â”€â”€ assets/           # Recursos estÃ¡ticos
â”œâ”€â”€ sessions/             # Sesiones guardadas
â””â”€â”€ uploads/             # Archivos temporales
```

---

## ğŸ¯ Modos de Uso

### 1. **Modo AutomÃ¡tico** (`/auto`)
- GrabaciÃ³n/subida simple
- TranscripciÃ³n automÃ¡tica
- Resumen inteligente
- Ideal para reuniones rÃ¡pidas

### 2. **Modo Profesional** (`/pro`)
- Control total del proceso
- GestiÃ³n de sesiones
- MÃºltiples opciones de anÃ¡lisis
- Ideal para reuniones importantes

### 3. **AnÃ¡lisis Avanzado** (`/resumen`)
- MÃºltiples tipos de anÃ¡lisis
- ResÃºmenes ejecutivos
- Planes de acciÃ³n
- AnÃ¡lisis de sentimientos

---

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno

Crea un archivo `.env`:
```bash
# Puerto del servidor
PORT=3000

# Modelos de IA
OLLAMA_MODEL=gemma2:2b
WHISPER_MODEL=base
WHISPER_LANGUAGE=es

# ConfiguraciÃ³n de archivos
MAX_FILE_SIZE=50mb
SESSION_RETENTION_DAYS=30

# Cloudflare (opcional)
CLOUDFLARE_TOKEN=tu_token_aqui
```

### Personalizar Prompts de IA

Edita `server.js` lÃ­neas ~200-250 para modificar los prompts de anÃ¡lisis.

---

## ğŸ› SoluciÃ³n de Problemas

### Error: "Ollama no encontrado"
```bash
# Verificar instalaciÃ³n
ollama --version

# Iniciar servicio
ollama serve

# Descargar modelo
ollama pull gemma2:2b
```

### Error: "Whisper no encontrado"
```bash
# Reinstalar Whisper
pip3 uninstall openai-whisper
pip3 install openai-whisper

# Verificar FFmpeg
ffmpeg -version
```

### Error: "Puerto 3000 ocupado"
```bash
# Encontrar proceso
lsof -ti:3000

# Terminar proceso
kill -9 $(lsof -ti:3000)
```

### Error: "Archivo muy grande"
Edita `server.js` lÃ­nea ~50:
```javascript
app.use(express.json({ limit: '100mb' }));
```

---

## ğŸ“Š Monitoreo y Logs

### Ver logs en tiempo real:
```bash
tail -f logs/app.log
```

### Verificar uso de recursos:
```bash
# CPU y memoria
htop

# Uso de GPU (si aplica)
nvidia-smi
```

---

## ğŸ”’ Seguridad

### Configurar HTTPS (ProducciÃ³n):
```bash
# Generar certificados SSL
openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365

# Editar server.js para usar HTTPS
```

### Configurar Firewall:
```bash
# Ubuntu/Debian
sudo ufw allow 3000
sudo ufw enable
```

---

## ğŸ”„ Actualizaciones

### Actualizar LA AURORA:
```bash
git pull origin main
npm install
```

### Actualizar Modelos:
```bash
# Ollama
ollama pull gemma2:2b

# Whisper se actualiza automÃ¡ticamente
pip3 install --upgrade openai-whisper
```

---

## ğŸ“ Soporte

### Logs de Debug:
```bash
# Activar modo debug
export DEBUG=true
node server.js
```

### Reportar Issues:
- GitHub: [Issues](https://github.com/joelosiris11/la-aurora/issues)
- Email: soporte@la-aurora.com

---

## ğŸ“„ Licencia

MIT License - Ver archivo `LICENSE` para mÃ¡s detalles.

---

## ğŸ‰ Â¡Listo!

Tu instalaciÃ³n de **LA AURORA** estÃ¡ completa. Accede a:

- **Local**: `http://localhost:3000`
- **PÃºblico**: Tu URL de Cloudflare Tunnel

### PrÃ³ximos Pasos:
1. Crea tu primera sesiÃ³n
2. Prueba ambos modos (AutomÃ¡tico y Profesional)
3. Experimenta con diferentes modelos de IA
4. Configura acceso pÃºblico si es necesario

Â¡Disfruta de tu sistema de reuniones con IA! ğŸš€