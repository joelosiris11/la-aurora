# üöÄ Gu√≠a de Instalaci√≥n R√°pida

## Variables de Entorno Necesarias

Crea un archivo `.env` en la ra√≠z del proyecto con:

```env
# Puerto del servidor (opcional, por defecto 3000)
PORT=3000

# URL de Ollama (opcional, por defecto http://localhost:11434)
OLLAMA_URL=http://localhost:11434

# Modelo de Ollama (usar el que tienes instalado)
OLLAMA_MODEL=gemma3:4b

# Modelo de Whisper local (tiny, base, small, medium, large)
WHISPER_MODEL=base
```

## ‚ú® ¬°Modelos 100% Locales!
- ‚ùå **NO necesitas** API Key de OpenAI 
- ‚úÖ Whisper funciona completamente local
- ‚úÖ Ollama funciona completamente local
- ‚úÖ Sin costo por uso
- ‚úÖ Privacidad total de tus datos

## Instalaci√≥n de Ollama

### macOS
```bash
brew install ollama
ollama serve
ollama pull llama3.2
```

### Linux
```bash
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull llama3.2
```

### Windows
1. Descargar desde https://ollama.ai
2. Instalar y ejecutar
3. En PowerShell: `ollama pull llama3.2`

## Iniciar la Aplicaci√≥n

```bash
# Instalar dependencias
npm install

# Iniciar en modo desarrollo
npm run dev

# O iniciar en producci√≥n
npm start
```

## Verificar que Todo Funciona

1. **Servidor**: http://localhost:3000 debe cargar la interfaz
2. **Ollama**: `ollama list` debe mostrar los modelos instalados
3. **Whisper**: Se descarga autom√°ticamente al usarse por primera vez
4. **Micr√≥fono**: El navegador debe pedir permisos de micr√≥fono

## Soluci√≥n R√°pida de Problemas

- **Error en Whisper**: El modelo se descarga autom√°ticamente (puede tardar la primera vez)
- **Error de conexi√≥n Ollama**: Ejecuta `ollama serve`
- **Sin permisos de micr√≥fono**: Usa HTTPS o localhost
- **Modelo no encontrado**: Ejecuta `ollama pull gemma3:4b`